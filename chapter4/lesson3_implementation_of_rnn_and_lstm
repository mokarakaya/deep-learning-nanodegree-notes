Character-wise RNNs:
The network will learn about a text one character at a time and then generate new text one character at a time.

Sequence batching:
One sequence -> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
Two sequences -> [1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12] -> batch size is 2
Lets say the sequence length is 3. Then the first batch will be [1, 2, 3], [7, 8, 9]

Implementing a Character-wise RNN:
Each character is encoded to a number.
y = next char of x -> Because, we'd like to predict next char of a given char.

LSTM Cell:
input -> LSTM -> LSTM -> softmax -> output
We create tf.contrib.rnn.BasicLSTMCell
with a DropoutWrapper then we give it to MultiRNNCell.
initial state may be cell.zero_state.

RNN Ouput:

Network Loss:

Building the network:
